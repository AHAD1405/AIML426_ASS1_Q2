import random
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
import matplotlib.pyplot as plt
import pandas as pd
import math
import time

def load_data(column_file_name, data_file_name):
    """
        Extract data from file and return datset
    """
    # Extract Column names --------------------------------------
    columns_name = []
    with open(column_file_name,'r') as columns_file: 
        columns = columns_file.readlines()
        for idx, line in enumerate(columns): # extract values
            if idx == 0: continue
            x = line.split()
            columns_name.append(x[0])
    columns_name.append('class')   # add column for target column

    # Extract data from file and create dataset -----------------
    dataset = pd.read_csv(data_file_name)
    dataset.columns = columns_name

    return dataset

def initial_pop(population_size, feature_num, seed_val):
    """
    Generate the initial population for the genetic algorithm.

    Args:
        population_size (int): The desired size of the initial population.
        feature_num (int): The number of the feature in dataset.

    Returns:
        list: A list of individuals, where each individual is a binary vector
              representing a potential solution. The length of each individual is equal to `feature_num`.
    """
    random.seed(seed_val)
    return [np.random.randint(2, size=feature_num) for _ in range(population_size)]

def classification(individual, dataset, target):
    """
        The fitness function evaluates the performance of the selected features:
    """
    selected_features = [index for index in range(len(individual)) if individual[index] == 1]
    if len(selected_features) == 0:
        return 0
    
    X_selected = dataset[:, selected_features]
    X_train, X_test, y_train, y_test = train_test_split(X_selected, target, test_size=0.3, random_state=42)
    clf = RandomForestClassifier(n_estimators=50)
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_test)

    return accuracy_score(y_test, predictions)

def WrapperGA(X, y):
    """
        -  In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, 
                we decide to add or remove features from the subset.

        -  Recursive Feature Elimination with Cross-Validated (RFECV) feature selection technique selects the best subset 
                of features for the estimator by removing 0 to N features iteratively using recursive feature elimination.
        
        -  Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. 
                Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features.
        
        -  
    """

    #min_features_to_select = 1  # Minimum number of features to consider
    clf = LogisticRegression()
    cv = StratifiedKFold(5)

    rfecv = RFECV(
        estimator=clf,
        step=1,
        cv=cv,
        scoring="accuracy",
        #min_features_to_select=min_features_to_select,
        n_jobs=2,
    )
    rfecv.fit(X, y)
    
    # Get the accuracy of the best subset of features
    accuracy = accuracy_score(y, rfecv.predict(X))

    # Names of selected features
    selected_features = X.columns[rfecv.support_] # support_: get boolean value for each feature, the feature with (True) means it selected feature
    return selected_features

def crossover(parent1, parent2, crossover_rate):
    """
        The function chooses a random crossover point, 
        then creates two new individuals by concatenating the genetic material of the two parents at that point
    """
    if np.random.rand() < crossover_rate:
        num_features = parent1.shape[0]
        crossover_point = np.random.randint(1, num_features - 1)
        child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
        return child1, child2
    else:
        return parent1, parent2

def mutation(individual, mutation_rate):
    """
    The function iterates over each element of the individual's genetic vector and, 
    with probability mutation_rate, flips the element (i.e., changes 0 to 1 or 1 to 0)

    PARAM: 
        - individual (list): A list representing the genetic vector or chromosome of an individual in the population.
        - items: A list of tuples, where each tuple represents an item with its weight and value (weight, value).
    RETURN:
        - mutated_individual (list): A new list representing the mutated individual's genetic vector after applying the mutation operation.
    """
    gens = individual.shape[0]
    mutated_individual = individual.copy()
    for i in range(gens):
        if random.random() < mutation_rate:
            mutated_individual[i] = 1 - mutated_individual[i]
    return mutated_individual

def create_table(WrapperGA, mean_, std_):
    """
    Create a dataset with two columns from two input lists.

    Args:
        list1 (list): The first list of values.
        list2 (list): The second list of values.
        column1_name (str): The name of the first column.
        column2_name (str): The name of the second column.

    Returns:
        pandas.DataFrame: A pandas DataFrame containing the two columns.
    """
    # Check if the lists have the same length
    #if len(WrapperGA) != len(FilterGA):
    #    raise ValueError("The input lists must have the same length.")

    # First column
    first_column = ['Run 1','Run 2','Run 3','Run 4','Run 5']

    # Create a dictionary with the two lists as values
    data = {'': first_column, 'WrapperGA': WrapperGA}

    # Create a pandas DataFrame from the dictionary
    data_table = pd.DataFrame(data)

    # Create a new DataFrame with the mean and concatenate it with (data_table)
    mean_row = pd.DataFrame({'': ['Mean'], 'WrapperGA': [mean_]})
    data_table = pd.concat([data_table, mean_row], ignore_index=True)

    # Create a new DataFrame with the stander deviation and concatenate it with (data_table)
    std_row = pd.DataFrame({'': ['STD'], 'WrapperGA': [std_] })
    data_table = pd.concat([data_table, std_row], ignore_index=True)

    return data_table

def calculate_stats(list_value):
    """
    Calculate the mean and standard deviation of a lists.

    Args:
        list_value (list): The first list of numbers.

    Returns:
        tuple: A tuple containing two elements:
            - mean1 (float): The mean of list.
            - std_dev1 (float): The standard deviation of list.
    """
    # Calculate the mean of list
    mean1 = sum(list_value) / len(list_value)

    # Calculate the standard deviation of list
    squared_diffs = [(x - mean1) ** 2 for x in list_value]
    variance1 = sum(squared_diffs) / len(list_value)
    std_dev1 = math.sqrt(variance1)

    return mean1, std_dev1

def main():
    # Genetic operation setting 
    population_size = 50
    generations = 5
    mutation_rate = 0.2
    crossover_rate = 0.5
    seed_value = [20, 30, 40, 50, 60]
    run_no = 5
    competition_time_li = []
    acc_li = []
    # dataset file name
    data_file =  'sonar.data'    # wbcd.data  , sonar.data
    column_file = 'sonar.names'  # wbcd.names , sonar.names
    
    # load data
    dataset = load_data(column_file, data_file)
    feature = dataset.iloc[:,:-1]
    target = dataset.iloc[:,-1]

    for run in range(run_no):
        start_time = time.time()  # Start timer for this run

        # FEATURE FITNESS FUNCTION: 
        WrapperGA_fs = WrapperGA(feature, target)  # Wrapper-based feature selection 

        # DATA TASFORMATION: create new dataset for each filter approach, created datset that only contain selected feature 
        WrapperGA_datset = dataset[:][list(WrapperGA_fs.array) + ['class']]

        # INITIALIZATION: Initialize the population 
        WrapperGA_population = initial_pop(population_size, WrapperGA_datset.iloc[:, :-1].shape[1], seed_value[run])

        # Apply Selection process
        for generation in range(generations):

            # EVALUATE: clasification and evaluates the performance of the selected features
            WrapperGA_fitness = [classification(individual, WrapperGA_datset.iloc[:,:-1].values, WrapperGA_datset.iloc[:,-1].values) for individual in WrapperGA_population]

            WrapperGA_parents = np.array(WrapperGA_population)[np.argsort(WrapperGA_fitness)][-2:]

            # CROSSOVER: Perform crossover to generate new offspring
            WrapperGA_offspring = crossover(WrapperGA_parents[0], WrapperGA_parents[1], crossover_rate)

            # MUTATION: Perform mutation on the new offspring
            WrapperGA_offspring = [mutation(_, mutation_rate) for _ in WrapperGA_offspring]

            # REPLACEMENT: Replace the least fit individuals with the new offspring
            WrapperGA_population = np.vstack((WrapperGA_population, WrapperGA_offspring))
            WrapperGA_fitness = np.array(WrapperGA_fitness)
            WrapperGA_population = WrapperGA_population[np.argsort(WrapperGA_fitness)][:-2]
            WrapperGA_population = np.vstack((WrapperGA_population, WrapperGA_offspring))
        
        end_time = time.time()  # End timer for this run
        competitive_time = end_time - start_time  # Calculate competitive time for this run
        competition_time_li.append(competitive_time)
        acc_li.append(max(WrapperGA_fitness))

    # Create a table and calculate mean and STD
    acc_mean, acc_std = calculate_stats(acc_li)
    acc_li = [round(x, 2) for x in acc_li]

    competitive_time_mean, competitive_time_std = calculate_stats(competition_time_li)
    competition_time_li = [round(x, 2) for x in competition_time_li]
    
    acc_table = create_table(acc_li, round(acc_mean), round(acc_std))
    competitive_time_table = create_table(competition_time_li, round(competitive_time_mean), round(competitive_time_std))

    # Print the best fitness values for each generation
    #print('Generation {}: WrapperGA best fitness = {:.4f}'.format(generation, max(WrapperGA_fitness)))
    #print(competition_time_li)
    print(f'Accuracy Table\n {acc_table}')
    print(f'Competional Time Table:\n {competitive_time_table}')

if __name__ == "__main__":
    main()